{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from qtorch_plus.quant import posit_quantize\n",
    "import requests\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset directory '/localsdd/yg9bq/LLM_quantization/imagenet_val' not found. Downloading ImageNet Mini...\n",
      "Download completed.\n"
     ]
    },
    {
     "ename": "BadZipFile",
     "evalue": "File is not a zip file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Extract the dataset\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mzipfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzip_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m zip_ref:\n\u001b[1;32m     19\u001b[0m     zip_ref\u001b[38;5;241m.\u001b[39mextractall(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Move the validation dataset to the target directory\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/posit/lib/python3.10/zipfile.py:1272\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[1;32m   1270\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m-> 1272\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_RealGetContents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1273\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m   1274\u001b[0m         \u001b[38;5;66;03m# set the modified flag so central directory gets written\u001b[39;00m\n\u001b[1;32m   1275\u001b[0m         \u001b[38;5;66;03m# even if no files are added to the archive\u001b[39;00m\n\u001b[1;32m   1276\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_didModify \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/posit/lib/python3.10/zipfile.py:1339\u001b[0m, in \u001b[0;36mZipFile._RealGetContents\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1337\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadZipFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile is not a zip file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m endrec:\n\u001b[0;32m-> 1339\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadZipFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile is not a zip file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1341\u001b[0m     \u001b[38;5;28mprint\u001b[39m(endrec)\n",
      "\u001b[0;31mBadZipFile\u001b[0m: File is not a zip file"
     ]
    }
   ],
   "source": [
    "# Set up a local path for the dataset\n",
    "dataset_dir = \"/localsdd/yg9bq/LLM_quantization/imagenet_val\"  # Target directory for the validation dataset\n",
    "\n",
    "# Ensure the dataset directory exists and contains the required files\n",
    "if not os.path.exists(dataset_dir):\n",
    "    print(f\"Dataset directory '{dataset_dir}' not found. Downloading ImageNet Mini...\")\n",
    "    \n",
    "    # Download ImageNet Mini\n",
    "    url = \"https://github.com/Divya-John/imagenet-mini/archive/refs/heads/main.zip\"\n",
    "    zip_path = \"./imagenet-mini.zip\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open(zip_path, \"wb\") as file:\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            file.write(chunk)\n",
    "    print(\"Download completed.\")\n",
    "    \n",
    "    # Extract the dataset\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(\"./\")\n",
    "    \n",
    "    # Move the validation dataset to the target directory\n",
    "    os.rename(\"./imagenet-mini-main/val\", dataset_dir)\n",
    "    os.rmdir(\"./imagenet-mini-main\")  # Clean up the unzipped folder\n",
    "    os.remove(zip_path)  # Clean up the zip file\n",
    "    print(f\"Dataset extracted to '{dataset_dir}'.\")\n",
    "\n",
    "# Define the dataset and preprocessing\n",
    "print(\"Loading ImageNet dataset...\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "imagenet_dataset = torchvision.datasets.ImageFolder(dataset_dir, transform=transform)\n",
    "val_loader = DataLoader(imagenet_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "print(\"ImageNet validation dataset loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ResNet-100 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /home/yg9bq/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n",
      "100%|██████████| 171M/171M [00:03<00:00, 57.4MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizing layer conv1\n",
      "Quantizing layer layer1.0.conv1\n",
      "Quantizing layer layer1.0.conv2\n",
      "Quantizing layer layer1.0.conv3\n",
      "Quantizing layer layer1.0.downsample.0\n",
      "Quantizing layer layer1.1.conv1\n",
      "Quantizing layer layer1.1.conv2\n",
      "Quantizing layer layer1.1.conv3\n",
      "Quantizing layer layer1.2.conv1\n",
      "Quantizing layer layer1.2.conv2\n",
      "Quantizing layer layer1.2.conv3\n",
      "Quantizing layer layer2.0.conv1\n",
      "Quantizing layer layer2.0.conv2\n",
      "Quantizing layer layer2.0.conv3\n",
      "Quantizing layer layer2.0.downsample.0\n",
      "Quantizing layer layer2.1.conv1\n",
      "Quantizing layer layer2.1.conv2\n",
      "Quantizing layer layer2.1.conv3\n",
      "Quantizing layer layer2.2.conv1\n",
      "Quantizing layer layer2.2.conv2\n",
      "Quantizing layer layer2.2.conv3\n",
      "Quantizing layer layer2.3.conv1\n",
      "Quantizing layer layer2.3.conv2\n",
      "Quantizing layer layer2.3.conv3\n",
      "Quantizing layer layer3.0.conv1\n",
      "Quantizing layer layer3.0.conv2\n",
      "Quantizing layer layer3.0.conv3\n",
      "Quantizing layer layer3.0.downsample.0\n",
      "Quantizing layer layer3.1.conv1\n",
      "Quantizing layer layer3.1.conv2\n",
      "Quantizing layer layer3.1.conv3\n",
      "Quantizing layer layer3.2.conv1\n",
      "Quantizing layer layer3.2.conv2\n",
      "Quantizing layer layer3.2.conv3\n",
      "Quantizing layer layer3.3.conv1\n",
      "Quantizing layer layer3.3.conv2\n",
      "Quantizing layer layer3.3.conv3\n",
      "Quantizing layer layer3.4.conv1\n",
      "Quantizing layer layer3.4.conv2\n",
      "Quantizing layer layer3.4.conv3\n",
      "Quantizing layer layer3.5.conv1\n",
      "Quantizing layer layer3.5.conv2\n",
      "Quantizing layer layer3.5.conv3\n",
      "Quantizing layer layer3.6.conv1\n",
      "Quantizing layer layer3.6.conv2\n",
      "Quantizing layer layer3.6.conv3\n",
      "Quantizing layer layer3.7.conv1\n",
      "Quantizing layer layer3.7.conv2\n",
      "Quantizing layer layer3.7.conv3\n",
      "Quantizing layer layer3.8.conv1\n",
      "Quantizing layer layer3.8.conv2\n",
      "Quantizing layer layer3.8.conv3\n",
      "Quantizing layer layer3.9.conv1\n",
      "Quantizing layer layer3.9.conv2\n",
      "Quantizing layer layer3.9.conv3\n",
      "Quantizing layer layer3.10.conv1\n",
      "Quantizing layer layer3.10.conv2\n",
      "Quantizing layer layer3.10.conv3\n",
      "Quantizing layer layer3.11.conv1\n",
      "Quantizing layer layer3.11.conv2\n",
      "Quantizing layer layer3.11.conv3\n",
      "Quantizing layer layer3.12.conv1\n",
      "Quantizing layer layer3.12.conv2\n",
      "Quantizing layer layer3.12.conv3\n",
      "Quantizing layer layer3.13.conv1\n",
      "Quantizing layer layer3.13.conv2\n",
      "Quantizing layer layer3.13.conv3\n",
      "Quantizing layer layer3.14.conv1\n",
      "Quantizing layer layer3.14.conv2\n",
      "Quantizing layer layer3.14.conv3\n",
      "Quantizing layer layer3.15.conv1\n",
      "Quantizing layer layer3.15.conv2\n",
      "Quantizing layer layer3.15.conv3\n",
      "Quantizing layer layer3.16.conv1\n",
      "Quantizing layer layer3.16.conv2\n",
      "Quantizing layer layer3.16.conv3\n",
      "Quantizing layer layer3.17.conv1\n",
      "Quantizing layer layer3.17.conv2\n",
      "Quantizing layer layer3.17.conv3\n",
      "Quantizing layer layer3.18.conv1\n",
      "Quantizing layer layer3.18.conv2\n",
      "Quantizing layer layer3.18.conv3\n",
      "Quantizing layer layer3.19.conv1\n",
      "Quantizing layer layer3.19.conv2\n",
      "Quantizing layer layer3.19.conv3\n",
      "Quantizing layer layer3.20.conv1\n",
      "Quantizing layer layer3.20.conv2\n",
      "Quantizing layer layer3.20.conv3\n",
      "Quantizing layer layer3.21.conv1\n",
      "Quantizing layer layer3.21.conv2\n",
      "Quantizing layer layer3.21.conv3\n",
      "Quantizing layer layer3.22.conv1\n",
      "Quantizing layer layer3.22.conv2\n",
      "Quantizing layer layer3.22.conv3\n",
      "Quantizing layer layer4.0.conv1\n",
      "Quantizing layer layer4.0.conv2\n",
      "Quantizing layer layer4.0.conv3\n",
      "Quantizing layer layer4.0.downsample.0\n",
      "Quantizing layer layer4.1.conv1\n",
      "Quantizing layer layer4.1.conv2\n",
      "Quantizing layer layer4.1.conv3\n",
      "Quantizing layer layer4.2.conv1\n",
      "Quantizing layer layer4.2.conv2\n",
      "Quantizing layer layer4.2.conv3\n",
      "Quantizing layer fc\n",
      "MAC operation count: 44442816\n",
      "Total layers quantized: 105\n"
     ]
    }
   ],
   "source": [
    "# Define quantization functions\n",
    "def linear_weight(input):\n",
    "    return posit_quantize(input, nsize=8, es=1, scale=1)\n",
    "\n",
    "def linear_activation(input):\n",
    "    return posit_quantize(input, nsize=8, es=1, scale=1)\n",
    "\n",
    "def forward_pre_hook_linear(m, input):\n",
    "    return (linear_activation(input[0]),)\n",
    "\n",
    "# Apply quantization to all layers\n",
    "layer_count = 0\n",
    "conv_count = 0\n",
    "op_count = 0\n",
    "\n",
    "for name, module in resnet100.named_modules():\n",
    "    if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "        layer_count += 1\n",
    "\n",
    "        # Quantize weights\n",
    "        print(f\"Quantizing layer {name}\")\n",
    "        module.weight.data = linear_weight(module.weight.data)\n",
    "\n",
    "        # Register forward hook for activations\n",
    "        module.register_forward_pre_hook(forward_pre_hook_linear)\n",
    "\n",
    "        # Count operations\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            conv_count += 1\n",
    "            op_count += module.weight.shape[0] * module.weight.shape[1] * module.weight.shape[2] * module.weight.shape[3]\n",
    "        elif isinstance(module, nn.Linear):\n",
    "            op_count += module.in_features * module.out_features\n",
    "\n",
    "print(\"MAC operation count:\", op_count)\n",
    "print(\"Total layers quantized:\", layer_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'path/to/sample_image.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Load a sample image and pass it through the quantized model\u001b[39;00m\n\u001b[1;32m     15\u001b[0m example_image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath/to/sample_image.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with an actual image path\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m input_image \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample_image_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Perform inference\u001b[39;00m\n\u001b[1;32m     19\u001b[0m resnet100\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m, in \u001b[0;36mpreprocess_image\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_image\u001b[39m(image_path):\n\u001b[1;32m      7\u001b[0m     transform \u001b[38;5;241m=\u001b[39m Compose([\n\u001b[1;32m      8\u001b[0m         ToTensor(),\n\u001b[1;32m      9\u001b[0m         Normalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])\n\u001b[1;32m     10\u001b[0m     ])\n\u001b[0;32m---> 11\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m transform(image)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/posit/lib/python3.10/site-packages/PIL/Image.py:3092\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3089\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[1;32m   3091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3092\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3093\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3095\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path/to/sample_image.jpg'"
     ]
    }
   ],
   "source": [
    "# Optionally test the quantized model on some data\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from PIL import Image\n",
    "\n",
    "# Example input image preprocessing\n",
    "def preprocess_image(image_path):\n",
    "    transform = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    return transform(image).unsqueeze(0).to(\"cuda\")\n",
    "\n",
    "# Load a sample image and pass it through the quantized model\n",
    "example_image_path = \"path/to/sample_image.jpg\"  # Replace with an actual image path\n",
    "input_image = preprocess_image(example_image_path)\n",
    "\n",
    "# Perform inference\n",
    "resnet100.eval()\n",
    "with torch.no_grad():\n",
    "    output = resnet100(input_image)\n",
    "    print(\"Model inference completed.\")\n",
    "    print(\"Output shape:\", output.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "posit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
