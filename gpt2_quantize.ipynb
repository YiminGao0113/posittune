{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating table for 2^(I.F), I=2, F=1 \n",
      "power 2 table:  [-4.0, -3.5, -3.0, -2.5, -2.0, -1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5]\n",
      "system 2^(2.1):  [0.0625, 0.08838834764831845, 0.125, 0.1767766952966369, 0.25, 0.3535533905932738, 0.5, 0.7071067811865476, 1.0, 1.4142135623730951, 2.0, 2.8284271247461903, 4.0, 5.656854249492381, 8.0, 11.313708498984761]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_log2system(int_length, fraction_length):\n",
    "  #system 2^(I.F)\n",
    "  print (\"generating table for 2^(I.F), I=%d, F=%d \"%(int_length,fraction_length))\n",
    "  step  =  2**(-fraction_length)\n",
    "  power_table = np.arange(-2**int_length,  2**int_length, step)\n",
    "  print (\"power 2 table: \", list(power_table))\n",
    "  table = list(map(lambda x: 2.0**x, power_table))\n",
    "  print (\"system 2^(%d.%d): \"%(int_length,fraction_length), table)\n",
    "  return table\n",
    "\n",
    "weight_table = generate_log2system(2,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov 18 17:46:32 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 520.61.05    Driver Version: 520.61.05    CUDA Version: 11.8     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:5E:00.0 Off |                  N/A |\n",
      "| 27%   31C    P8     3W / 250W |     20MiB /  8192MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2895      G   /usr/lib/xorg/Xorg                  9MiB |\n",
      "|    0   N/A  N/A      3104      G   /usr/bin/gnome-shell                6MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "Requirement already satisfied: transformers in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (4.45.2)\n",
      "Requirement already satisfied: datasets in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (3.0.2)\n",
      "Requirement already satisfied: filelock in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from transformers) (0.26.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/yg9bq/.local/lib/python3.10/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/yg9bq/.local/lib/python3.10/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from aiohttp->datasets) (1.16.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: nlp in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (0.4.0)\n",
      "Requirement already satisfied: numpy in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from nlp) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=0.16.0 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from nlp) (18.0.0)\n",
      "Requirement already satisfied: dill in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from nlp) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from nlp) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from nlp) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from nlp) (4.66.5)\n",
      "Requirement already satisfied: filelock in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from nlp) (3.13.1)\n",
      "Requirement already satisfied: xxhash in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from nlp) (3.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from requests>=2.19.0->nlp) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from requests>=2.19.0->nlp) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from requests>=2.19.0->nlp) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from requests>=2.19.0->nlp) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from pandas->nlp) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from pandas->nlp) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from pandas->nlp) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->nlp) (1.16.0)\n",
      "fatal: destination path 'QPyTorch' already exists and is not an empty directory.\n",
      "fatal: not a git repository (or any parent up to mount point /)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "\u001b[31mERROR: Directory './' is not installable. Neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: ninja in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (1.11.1.1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "!nvidia-smi\n",
    "!pip install transformers datasets\n",
    "!pip install nlp\n",
    "\n",
    "!git clone https://github.com/minhhn2910/QPyTorch\n",
    "try:\n",
    "  os.chdir('/content/QPyTorch')\n",
    "except:\n",
    "  pass\n",
    "!git checkout posit-constant-generation\n",
    "\n",
    "!pip install ./\n",
    "!pip install ninja\n",
    "\n",
    "try:\n",
    "  os.chdir('/content/')\n",
    "except:\n",
    "  pass\n",
    "\n",
    "import torch\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "  raise RuntimeError('Cannot run this cell without GPU runtime.')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/yg9bq/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/yg9bq/.cache/torch_extensions/py310_cu118/quant_cpu/build.ninja...\n",
      "Building extension module quant_cpu...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module quant_cpu...\n",
      "Using /home/yg9bq/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/yg9bq/.cache/torch_extensions/py310_cu118/quant_cuda/build.ninja...\n",
      "/home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module quant_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module quant_cuda...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "8\n",
      "[0.0065918  0.02197266 0.0402832  0.0625     0.08514404 0.12109375\n",
      " 0.2109375  0.53640747]\n",
      "[0.015625   0.0625     0.125      0.1640625  0.29296875 0.4375\n",
      " 0.625      0.84375    1.         1.25       1.5        2.11486816\n",
      " 3.08789062 4.25       6.         8.25      ]\n",
      "Quantizing the embeddings...\n",
      "Quantizing the embeddings...\n",
      "MAC operation count  772117760\n",
      "Layer count  145\n",
      "Machine learning is the study of artificial intelligence algorithms that learn something by observing data. Machine learning helps humans and technology work better together.\n",
      "\n",
      "What types of topics will I learn?\n",
      "\n",
      "In each course, you'll learn relevant topics in machine learning\n",
      "-----------------\n",
      "Machine learning is the study of data algorithms and they will be taught in the context of our Master's Research Design. Their thesis will be about improving the capacity of computers to search a vast quantity of documents in different languages, both in the public domain and\n",
      "-----------------------------------\n",
      "Machine learning is the study of how a set of rules can be combined such that the resulting strategy produces higher or lower performance than the simple set of rules alone.\n",
      "\n",
      "These days, we have a lot of different frameworks and algorithms for machine learning.\n",
      "-----------------------------------\n",
      "In the 19th century, the invention of the steam engine created an incredible surge in productivity and created an entire new industry: shipping.\n",
      "\n",
      "Steam transport was the first industrial revolution. The steam engine could not only drive a ship, it could also\n",
      "-----------------\n",
      "In the 19th century, the invention of an early telephone system meant that people could communicate, say, from London to Paris directly using an electric current and the land line network.\n",
      "\n",
      "Modern telephone networks are in general a lot more complicated. The\n",
      "-----------------------------------\n",
      "In the 19th century, the invention of chemical synthesis of chemicals transformed the world. Modern science was born.\n",
      "\n",
      "COPYRIGHT 2002-2017 by John C. McPhee. All Rights Reserved.\n",
      "\n",
      "Original publication (March 2003\n",
      "-----------------------------------\n",
      "A robot was created for the production line of the German manufacturer Daimler-Benz, making it possible to build \"cargo transport units (TCU),\" capable of taking part in such work as oilfield servicing. The robots have a high level\n",
      "-----------------\n",
      "A robot was created to simulate a cat - the 'CAT-bot - and it had to catch a ball.\n",
      "\n",
      "Kurashige Shizaka spent a year studying the movements of a real cat - the 'CAT-bot\n",
      "-----------------------------------\n",
      "A robot was created as a weapon of mass destruction during a space mission. After its creator is killed, it begins a program of destroying all intelligent life. It is said the evil god in question is a god of peace.\n",
      "\n",
      "Lupin\n",
      "-----------------------------------\n",
      "One day I will go to Africa and learn something about our history.\"\n",
      "\n",
      "The former Premier League star has made his name at Aston Villa (2007–2012) and for the last four years in Spain scoring five goals in 11 league games for Sev\n",
      "-----------------\n",
      "One day I will write an article regarding the real story of the Boston Bombings and how they were carried out using F-16 and the US Marines. I will tell the American people the complete truth about the events and the people of that time was\n",
      "-----------------------------------\n",
      "One day I will have a chance of seeing your new face.\n",
      "\n",
      "Thank you for your time.\"\n",
      "\n",
      "Terrify was stunned by the man's face when the phone call interrupted his conversation with a customer service representative from a company named T\n",
      "-----------------------------------\n",
      "\n",
      " FP32 ref : \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "device = 'cuda'\n",
    "model_id = 'gpt2-large'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_id)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_id)\n",
    "from transformers import pipeline\n",
    "from transformers import set_seed\n",
    "from qtorch_plus.quant import posit_quantize, float_quantize, configurable_table_quantize\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "\n",
    "\n",
    "#final with 3 layers skipped :\n",
    "full_table = np.array([6.59179688e-03, 2.19726562e-02, 4.02832031e-02, 6.25000000e-02,\n",
    "                8.51440430e-02, 2.10937500e-01, 1.21093750e-01, 5.36407471e-01,\n",
    "                1.56250000e-02, 6.25000000e-02, 1.25000000e-01, 1.64062500e-01,\n",
    "                2.92968750e-01, 6.25000000e-01, 8.43750000e-01, 4.37500000e-01,\n",
    "                1.00000000e+00, 2.11486816e+00, 1.50000000e+00, 1.25000000e+00,\n",
    "                4.25000000e+00, 3.08789062e+00, 8.25000000e+00, 6.00000000e+00])\n",
    "\n",
    "weight_table = full_table[:8]\n",
    "act_table = full_table[8:]\n",
    "weight_table = np.sort(weight_table)\n",
    "act_table = np.sort(act_table)\n",
    "print (len(weight_table))\n",
    "print (weight_table)\n",
    "print (act_table)\n",
    "\n",
    "def linear_weight(input):\n",
    "    # return input\n",
    "    # return configurable_table_quantize(input, torch.tensor(weight_table,dtype = torch.float), scale= 1.0)\n",
    "    return posit_quantize(input,nsize=8, es=1, scale = 1)\n",
    "    # return float_quantize(input,exp=4, man=3, rounding=\"nearest\")\n",
    "\n",
    "def linear_activation(input):\n",
    "    # return input\n",
    "    # return configurable_table_quantize(input,torch.tensor(act_table, dtype=torch.float), scale= 1.0)\n",
    "    return posit_quantize(input,nsize=8, es=1, scale = 1)\n",
    "    # return float_quantize(input,exp=4, man=3, rounding=\"nearest\")\n",
    "\n",
    "def forward_pre_hook_linear(m, input):\n",
    "    return (linear_activation(input[0]),)\n",
    "\n",
    "model = model.to(device)\n",
    "layer_count = 0\n",
    "linear_layer_count = 0\n",
    "op_count = 0\n",
    "import transformers.modeling_utils as  modeling_utils\n",
    "for name, module in model.named_modules():\n",
    "  # print (type(module))\n",
    "  if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear) or isinstance(module, modeling_utils.Conv1D):\n",
    "    #print (name)\n",
    "    layer_count = layer_count + 1\n",
    "    #if ('lm_head' not in name and 'h.0' not in name):\n",
    "    # print(f\"\\nBefore quantization - {name} weights:\", module.weight.data.flatten()[:8])\n",
    "    module.weight.data = linear_weight(module.weight.data)\n",
    "    # print(f\"After quantization - {name} weights:\", module.weight.data.flatten()[:8])\n",
    "    module.register_forward_pre_hook(forward_pre_hook_linear)\n",
    "\n",
    "    if (isinstance(module, modeling_utils.Conv1D)):\n",
    "      #print (module.weight.shape)\n",
    "      op_count = op_count + module.weight.shape[0] *module.weight.shape[1]\n",
    "    else:\n",
    "      op_count = op_count + module.in_features*module.out_features\n",
    "  elif isinstance(module, nn.Embedding):\n",
    "    module.weight.data = linear_weight(module.weight.data)\n",
    "    print('Quantizing the embeddings...')\n",
    "print (\"MAC operation count \", op_count)\n",
    "print (\"Layer count \", layer_count)\n",
    "\n",
    "\n",
    "# from nlp import load_dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "test = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
    "encodings = tokenizer('\\n\\n'.join(test['text']), return_tensors='pt')\n",
    "#model = model.to(device)\n",
    "def generate_text(model_new):\n",
    "  text_generation = pipeline(\"text-generation\", model=model_new, tokenizer=tokenizer, device = 0)\n",
    "  #set_seed(42)\n",
    "  prefix_texts = [\"Machine learning is the study of\",\n",
    "                    \"In the 19th century, the invention\",\n",
    "                    \"A robot was created\",\n",
    "                    \"One day I will\"\n",
    "                  ]\n",
    "  for prefix_text in prefix_texts:\n",
    "    #generated_text= text_generation(prefix_text, max_length=50, do_sample=False )[0]\n",
    "    set_seed(42)\n",
    "    generated_text= text_generation(prefix_text, max_length=50, num_return_sequences=3)\n",
    "    print(generated_text[0]['generated_text'])\n",
    "    print (\"-----------------\")\n",
    "    print(generated_text[1]['generated_text'])\n",
    "    print (\"-----------------------------------\")\n",
    "    print(generated_text[2]['generated_text'])\n",
    "    print (\"-----------------------------------\")\n",
    "generate_text(model)\n",
    "\n",
    "print (\"\\n FP32 ref : \\n\")\n",
    "# model = GPT2LMHeadModel.from_pretrained(model_id).to(device)\n",
    "# generate_text(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size: 287644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 281/281 [01:03<00:00,  4.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 19.477630615234375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "max_length = model.config.n_positions\n",
    "stride = 1024\n",
    "\n",
    "lls = []\n",
    "input_size = encodings.input_ids.size(1)\n",
    "print(\"Input size:\", input_size)\n",
    "\n",
    "for i in tqdm(range(0, input_size, stride)):\n",
    "    begin_loc = max(i + stride - max_length, 0)\n",
    "    end_loc = min(i + stride, input_size)\n",
    "    trg_len = end_loc - i\n",
    "\n",
    "    input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "    target_ids = input_ids.clone()\n",
    "    target_ids[:, :-trg_len] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "        log_likelihood = outputs[0] * trg_len\n",
    "        # print(f\"Log Likelihood for step {i}: {log_likelihood}\")\n",
    "\n",
    "    lls.append(log_likelihood)\n",
    "\n",
    "if lls:  # Ensure lls is not empty\n",
    "    ppl = torch.exp(torch.stack(lls).sum() / input_size)\n",
    "    print(\"Perplexity:\", ppl.item())\n",
    "else:\n",
    "    print(\"No log likelihoods calculated.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "posit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
