{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating table for 2^(I.F), I=2, F=1 \n",
      "power 2 table:  [-4.0, -3.5, -3.0, -2.5, -2.0, -1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5]\n",
      "system 2^(2.1):  [0.0625, 0.08838834764831845, 0.125, 0.1767766952966369, 0.25, 0.3535533905932738, 0.5, 0.7071067811865476, 1.0, 1.4142135623730951, 2.0, 2.8284271247461903, 4.0, 5.656854249492381, 8.0, 11.313708498984761]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_log2system(int_length, fraction_length):\n",
    "  #system 2^(I.F)\n",
    "  print (\"generating table for 2^(I.F), I=%d, F=%d \"%(int_length,fraction_length))\n",
    "  step  =  2**(-fraction_length)\n",
    "  power_table = np.arange(-2**int_length,  2**int_length, step)\n",
    "  print (\"power 2 table: \", list(power_table))\n",
    "  table = list(map(lambda x: 2.0**x, power_table))\n",
    "  print (\"system 2^(%d.%d): \"%(int_length,fraction_length), table)\n",
    "  return table\n",
    "\n",
    "weight_table = generate_log2system(2,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov 18 22:29:00 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 520.61.05    Driver Version: 520.61.05    CUDA Version: 11.8     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:5E:00.0 Off |                  N/A |\n",
      "| 26%   35C    P5    20W / 250W |     18MiB /  8192MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2895      G   /usr/lib/xorg/Xorg                  9MiB |\n",
      "|    0   N/A  N/A      3104      G   /usr/bin/gnome-shell                6MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "Requirement already satisfied: transformers in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (4.45.2)\n",
      "Requirement already satisfied: datasets in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (3.0.2)\n",
      "Requirement already satisfied: filelock in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from transformers) (0.26.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/yg9bq/.local/lib/python3.10/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/yg9bq/.local/lib/python3.10/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from aiohttp->datasets) (1.16.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: nlp in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (0.4.0)\n",
      "Requirement already satisfied: numpy in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from nlp) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=0.16.0 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from nlp) (18.0.0)\n",
      "Requirement already satisfied: dill in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from nlp) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from nlp) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from nlp) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from nlp) (4.66.5)\n",
      "Requirement already satisfied: filelock in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from nlp) (3.13.1)\n",
      "Requirement already satisfied: xxhash in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from nlp) (3.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from requests>=2.19.0->nlp) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from requests>=2.19.0->nlp) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from requests>=2.19.0->nlp) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from requests>=2.19.0->nlp) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from pandas->nlp) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from pandas->nlp) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from pandas->nlp) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->nlp) (1.16.0)\n",
      "fatal: destination path 'QPyTorch' already exists and is not an empty directory.\n",
      "fatal: not a git repository (or any parent up to mount point /)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "\u001b[31mERROR: Directory './' is not installable. Neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: ninja in /home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages (1.11.1.1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "!nvidia-smi\n",
    "!pip install transformers datasets\n",
    "!pip install nlp\n",
    "\n",
    "!git clone https://github.com/minhhn2910/QPyTorch\n",
    "try:\n",
    "  os.chdir('/content/QPyTorch')\n",
    "except:\n",
    "  pass\n",
    "!git checkout posit-constant-generation\n",
    "\n",
    "!pip install ./\n",
    "!pip install ninja\n",
    "\n",
    "try:\n",
    "  os.chdir('/content/')\n",
    "except:\n",
    "  pass\n",
    "\n",
    "import torch\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "  raise RuntimeError('Cannot run this cell without GPU runtime.')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing all layers in the model:\n",
      " : <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>\n",
      "transformer : <class 'transformers.models.gpt2.modeling_gpt2.GPT2Model'>\n",
      "transformer.wte : <class 'torch.nn.modules.sparse.Embedding'>\n",
      "transformer.wpe : <class 'torch.nn.modules.sparse.Embedding'>\n",
      "transformer.drop : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h : <class 'torch.nn.modules.container.ModuleList'>\n",
      "transformer.h.0 : <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
      "transformer.h.0.ln_1 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.0.attn : <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\n",
      "transformer.h.0.attn.c_attn : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.0.attn.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.0.attn.attn_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.0.attn.resid_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.0.ln_2 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.0.mlp : <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
      "transformer.h.0.mlp.c_fc : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.0.mlp.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.0.mlp.act : <class 'transformers.activations.NewGELUActivation'>\n",
      "transformer.h.0.mlp.dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.1 : <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
      "transformer.h.1.ln_1 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.1.attn : <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\n",
      "transformer.h.1.attn.c_attn : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.1.attn.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.1.attn.attn_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.1.attn.resid_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.1.ln_2 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.1.mlp : <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
      "transformer.h.1.mlp.c_fc : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.1.mlp.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.1.mlp.act : <class 'transformers.activations.NewGELUActivation'>\n",
      "transformer.h.1.mlp.dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.2 : <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
      "transformer.h.2.ln_1 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.2.attn : <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\n",
      "transformer.h.2.attn.c_attn : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.2.attn.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.2.attn.attn_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.2.attn.resid_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.2.ln_2 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.2.mlp : <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
      "transformer.h.2.mlp.c_fc : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.2.mlp.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.2.mlp.act : <class 'transformers.activations.NewGELUActivation'>\n",
      "transformer.h.2.mlp.dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.3 : <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
      "transformer.h.3.ln_1 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.3.attn : <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\n",
      "transformer.h.3.attn.c_attn : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.3.attn.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.3.attn.attn_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.3.attn.resid_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.3.ln_2 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.3.mlp : <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
      "transformer.h.3.mlp.c_fc : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.3.mlp.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.3.mlp.act : <class 'transformers.activations.NewGELUActivation'>\n",
      "transformer.h.3.mlp.dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.4 : <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
      "transformer.h.4.ln_1 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.4.attn : <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\n",
      "transformer.h.4.attn.c_attn : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.4.attn.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.4.attn.attn_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.4.attn.resid_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.4.ln_2 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.4.mlp : <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
      "transformer.h.4.mlp.c_fc : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.4.mlp.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.4.mlp.act : <class 'transformers.activations.NewGELUActivation'>\n",
      "transformer.h.4.mlp.dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.5 : <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
      "transformer.h.5.ln_1 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.5.attn : <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\n",
      "transformer.h.5.attn.c_attn : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.5.attn.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.5.attn.attn_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.5.attn.resid_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.5.ln_2 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.5.mlp : <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
      "transformer.h.5.mlp.c_fc : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.5.mlp.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.5.mlp.act : <class 'transformers.activations.NewGELUActivation'>\n",
      "transformer.h.5.mlp.dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.6 : <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
      "transformer.h.6.ln_1 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.6.attn : <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\n",
      "transformer.h.6.attn.c_attn : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.6.attn.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.6.attn.attn_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.6.attn.resid_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.6.ln_2 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.6.mlp : <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
      "transformer.h.6.mlp.c_fc : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.6.mlp.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.6.mlp.act : <class 'transformers.activations.NewGELUActivation'>\n",
      "transformer.h.6.mlp.dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.7 : <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
      "transformer.h.7.ln_1 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.7.attn : <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\n",
      "transformer.h.7.attn.c_attn : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.7.attn.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.7.attn.attn_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.7.attn.resid_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.7.ln_2 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.7.mlp : <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
      "transformer.h.7.mlp.c_fc : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.7.mlp.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.7.mlp.act : <class 'transformers.activations.NewGELUActivation'>\n",
      "transformer.h.7.mlp.dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.8 : <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
      "transformer.h.8.ln_1 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.8.attn : <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\n",
      "transformer.h.8.attn.c_attn : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.8.attn.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.8.attn.attn_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.8.attn.resid_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.8.ln_2 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.8.mlp : <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
      "transformer.h.8.mlp.c_fc : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.8.mlp.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.8.mlp.act : <class 'transformers.activations.NewGELUActivation'>\n",
      "transformer.h.8.mlp.dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.9 : <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
      "transformer.h.9.ln_1 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.9.attn : <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\n",
      "transformer.h.9.attn.c_attn : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.9.attn.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.9.attn.attn_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.9.attn.resid_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.9.ln_2 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.9.mlp : <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
      "transformer.h.9.mlp.c_fc : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.9.mlp.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.9.mlp.act : <class 'transformers.activations.NewGELUActivation'>\n",
      "transformer.h.9.mlp.dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.10 : <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
      "transformer.h.10.ln_1 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.10.attn : <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\n",
      "transformer.h.10.attn.c_attn : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.10.attn.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.10.attn.attn_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.10.attn.resid_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.10.ln_2 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.10.mlp : <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
      "transformer.h.10.mlp.c_fc : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.10.mlp.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.10.mlp.act : <class 'transformers.activations.NewGELUActivation'>\n",
      "transformer.h.10.mlp.dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.11 : <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
      "transformer.h.11.ln_1 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.11.attn : <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\n",
      "transformer.h.11.attn.c_attn : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.11.attn.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.11.attn.attn_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.11.attn.resid_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.11.ln_2 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.11.mlp : <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
      "transformer.h.11.mlp.c_fc : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.11.mlp.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.11.mlp.act : <class 'transformers.activations.NewGELUActivation'>\n",
      "transformer.h.11.mlp.dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.12 : <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
      "transformer.h.12.ln_1 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.12.attn : <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\n",
      "transformer.h.12.attn.c_attn : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.12.attn.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.12.attn.attn_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.12.attn.resid_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.12.ln_2 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.12.mlp : <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
      "transformer.h.12.mlp.c_fc : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.12.mlp.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.12.mlp.act : <class 'transformers.activations.NewGELUActivation'>\n",
      "transformer.h.12.mlp.dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.13 : <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
      "transformer.h.13.ln_1 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.13.attn : <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\n",
      "transformer.h.13.attn.c_attn : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.13.attn.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.13.attn.attn_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.13.attn.resid_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.13.ln_2 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.13.mlp : <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
      "transformer.h.13.mlp.c_fc : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.13.mlp.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.13.mlp.act : <class 'transformers.activations.NewGELUActivation'>\n",
      "transformer.h.13.mlp.dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.14 : <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
      "transformer.h.14.ln_1 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.14.attn : <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\n",
      "transformer.h.14.attn.c_attn : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.14.attn.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.14.attn.attn_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.14.attn.resid_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.14.ln_2 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.14.mlp : <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
      "transformer.h.14.mlp.c_fc : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.14.mlp.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.14.mlp.act : <class 'transformers.activations.NewGELUActivation'>\n",
      "transformer.h.14.mlp.dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.15 : <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
      "transformer.h.15.ln_1 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.15.attn : <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\n",
      "transformer.h.15.attn.c_attn : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.15.attn.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.15.attn.attn_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.15.attn.resid_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.15.ln_2 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.15.mlp : <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
      "transformer.h.15.mlp.c_fc : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.15.mlp.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.15.mlp.act : <class 'transformers.activations.NewGELUActivation'>\n",
      "transformer.h.15.mlp.dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.16 : <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
      "transformer.h.16.ln_1 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.16.attn : <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\n",
      "transformer.h.16.attn.c_attn : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.16.attn.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.16.attn.attn_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.16.attn.resid_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.16.ln_2 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.16.mlp : <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
      "transformer.h.16.mlp.c_fc : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.16.mlp.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.16.mlp.act : <class 'transformers.activations.NewGELUActivation'>\n",
      "transformer.h.16.mlp.dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.17 : <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
      "transformer.h.17.ln_1 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.17.attn : <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\n",
      "transformer.h.17.attn.c_attn : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.17.attn.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.17.attn.attn_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.17.attn.resid_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.17.ln_2 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.17.mlp : <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
      "transformer.h.17.mlp.c_fc : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.17.mlp.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.17.mlp.act : <class 'transformers.activations.NewGELUActivation'>\n",
      "transformer.h.17.mlp.dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.18 : <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
      "transformer.h.18.ln_1 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.18.attn : <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\n",
      "transformer.h.18.attn.c_attn : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.18.attn.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.18.attn.attn_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.18.attn.resid_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.18.ln_2 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.18.mlp : <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
      "transformer.h.18.mlp.c_fc : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.18.mlp.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.18.mlp.act : <class 'transformers.activations.NewGELUActivation'>\n",
      "transformer.h.18.mlp.dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.19 : <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
      "transformer.h.19.ln_1 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.19.attn : <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\n",
      "transformer.h.19.attn.c_attn : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.19.attn.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.19.attn.attn_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.19.attn.resid_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.19.ln_2 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.19.mlp : <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
      "transformer.h.19.mlp.c_fc : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.19.mlp.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.19.mlp.act : <class 'transformers.activations.NewGELUActivation'>\n",
      "transformer.h.19.mlp.dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.20 : <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
      "transformer.h.20.ln_1 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.20.attn : <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\n",
      "transformer.h.20.attn.c_attn : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.20.attn.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.20.attn.attn_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.20.attn.resid_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.20.ln_2 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.20.mlp : <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
      "transformer.h.20.mlp.c_fc : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.20.mlp.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.20.mlp.act : <class 'transformers.activations.NewGELUActivation'>\n",
      "transformer.h.20.mlp.dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.21 : <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
      "transformer.h.21.ln_1 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.21.attn : <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\n",
      "transformer.h.21.attn.c_attn : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.21.attn.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.21.attn.attn_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.21.attn.resid_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.21.ln_2 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.21.mlp : <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
      "transformer.h.21.mlp.c_fc : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.21.mlp.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.21.mlp.act : <class 'transformers.activations.NewGELUActivation'>\n",
      "transformer.h.21.mlp.dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.22 : <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
      "transformer.h.22.ln_1 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.22.attn : <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\n",
      "transformer.h.22.attn.c_attn : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.22.attn.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.22.attn.attn_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.22.attn.resid_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.22.ln_2 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.22.mlp : <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
      "transformer.h.22.mlp.c_fc : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.22.mlp.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.22.mlp.act : <class 'transformers.activations.NewGELUActivation'>\n",
      "transformer.h.22.mlp.dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.23 : <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
      "transformer.h.23.ln_1 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.23.attn : <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\n",
      "transformer.h.23.attn.c_attn : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.23.attn.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.23.attn.attn_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.23.attn.resid_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.23.ln_2 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.23.mlp : <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
      "transformer.h.23.mlp.c_fc : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.23.mlp.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.23.mlp.act : <class 'transformers.activations.NewGELUActivation'>\n",
      "transformer.h.23.mlp.dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.24 : <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
      "transformer.h.24.ln_1 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.24.attn : <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\n",
      "transformer.h.24.attn.c_attn : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.24.attn.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.24.attn.attn_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.24.attn.resid_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.24.ln_2 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.24.mlp : <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
      "transformer.h.24.mlp.c_fc : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.24.mlp.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.24.mlp.act : <class 'transformers.activations.NewGELUActivation'>\n",
      "transformer.h.24.mlp.dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.25 : <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
      "transformer.h.25.ln_1 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.25.attn : <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\n",
      "transformer.h.25.attn.c_attn : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.25.attn.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.25.attn.attn_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.25.attn.resid_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.25.ln_2 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.25.mlp : <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
      "transformer.h.25.mlp.c_fc : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.25.mlp.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.25.mlp.act : <class 'transformers.activations.NewGELUActivation'>\n",
      "transformer.h.25.mlp.dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.26 : <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
      "transformer.h.26.ln_1 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.26.attn : <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\n",
      "transformer.h.26.attn.c_attn : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.26.attn.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.26.attn.attn_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.26.attn.resid_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.26.ln_2 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.26.mlp : <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
      "transformer.h.26.mlp.c_fc : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.26.mlp.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.26.mlp.act : <class 'transformers.activations.NewGELUActivation'>\n",
      "transformer.h.26.mlp.dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.27 : <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
      "transformer.h.27.ln_1 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.27.attn : <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\n",
      "transformer.h.27.attn.c_attn : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.27.attn.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.27.attn.attn_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.27.attn.resid_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.27.ln_2 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.27.mlp : <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
      "transformer.h.27.mlp.c_fc : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.27.mlp.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.27.mlp.act : <class 'transformers.activations.NewGELUActivation'>\n",
      "transformer.h.27.mlp.dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.28 : <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
      "transformer.h.28.ln_1 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.28.attn : <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\n",
      "transformer.h.28.attn.c_attn : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.28.attn.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.28.attn.attn_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.28.attn.resid_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.28.ln_2 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.28.mlp : <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
      "transformer.h.28.mlp.c_fc : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.28.mlp.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.28.mlp.act : <class 'transformers.activations.NewGELUActivation'>\n",
      "transformer.h.28.mlp.dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.29 : <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
      "transformer.h.29.ln_1 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.29.attn : <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\n",
      "transformer.h.29.attn.c_attn : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.29.attn.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.29.attn.attn_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.29.attn.resid_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.29.ln_2 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.29.mlp : <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
      "transformer.h.29.mlp.c_fc : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.29.mlp.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.29.mlp.act : <class 'transformers.activations.NewGELUActivation'>\n",
      "transformer.h.29.mlp.dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.30 : <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
      "transformer.h.30.ln_1 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.30.attn : <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\n",
      "transformer.h.30.attn.c_attn : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.30.attn.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.30.attn.attn_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.30.attn.resid_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.30.ln_2 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.30.mlp : <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
      "transformer.h.30.mlp.c_fc : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.30.mlp.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.30.mlp.act : <class 'transformers.activations.NewGELUActivation'>\n",
      "transformer.h.30.mlp.dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.31 : <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
      "transformer.h.31.ln_1 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.31.attn : <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\n",
      "transformer.h.31.attn.c_attn : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.31.attn.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.31.attn.attn_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.31.attn.resid_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.31.ln_2 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.31.mlp : <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
      "transformer.h.31.mlp.c_fc : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.31.mlp.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.31.mlp.act : <class 'transformers.activations.NewGELUActivation'>\n",
      "transformer.h.31.mlp.dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.32 : <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
      "transformer.h.32.ln_1 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.32.attn : <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\n",
      "transformer.h.32.attn.c_attn : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.32.attn.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.32.attn.attn_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.32.attn.resid_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.32.ln_2 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.32.mlp : <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
      "transformer.h.32.mlp.c_fc : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.32.mlp.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.32.mlp.act : <class 'transformers.activations.NewGELUActivation'>\n",
      "transformer.h.32.mlp.dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.33 : <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
      "transformer.h.33.ln_1 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.33.attn : <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\n",
      "transformer.h.33.attn.c_attn : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.33.attn.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.33.attn.attn_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.33.attn.resid_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.33.ln_2 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.33.mlp : <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
      "transformer.h.33.mlp.c_fc : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.33.mlp.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.33.mlp.act : <class 'transformers.activations.NewGELUActivation'>\n",
      "transformer.h.33.mlp.dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.34 : <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
      "transformer.h.34.ln_1 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.34.attn : <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\n",
      "transformer.h.34.attn.c_attn : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.34.attn.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.34.attn.attn_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.34.attn.resid_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.34.ln_2 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.34.mlp : <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
      "transformer.h.34.mlp.c_fc : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.34.mlp.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.34.mlp.act : <class 'transformers.activations.NewGELUActivation'>\n",
      "transformer.h.34.mlp.dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.35 : <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
      "transformer.h.35.ln_1 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.35.attn : <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\n",
      "transformer.h.35.attn.c_attn : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.35.attn.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.35.attn.attn_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.35.attn.resid_dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.h.35.ln_2 : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.h.35.mlp : <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
      "transformer.h.35.mlp.c_fc : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.35.mlp.c_proj : <class 'transformers.pytorch_utils.Conv1D'>\n",
      "transformer.h.35.mlp.act : <class 'transformers.activations.NewGELUActivation'>\n",
      "transformer.h.35.mlp.dropout : <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.ln_f : <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "lm_head : <class 'torch.nn.modules.linear.Linear'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "device = 'cuda'\n",
    "model_id = 'gpt2-large'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_id)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_id)\n",
    "# Print all layer names\n",
    "print(\"Listing all layers in the model:\")\n",
    "for name, module in model.named_modules():\n",
    "    print(name, \":\", type(module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/yg9bq/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/yg9bq/.cache/torch_extensions/py310_cu118/quant_cpu/build.ninja...\n",
      "Building extension module quant_cpu...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module quant_cpu...\n",
      "Using /home/yg9bq/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/yg9bq/.cache/torch_extensions/py310_cu118/quant_cuda/build.ninja...\n",
      "/home/yg9bq/miniconda3/envs/posit/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building extension module quant_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module quant_cuda...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "[0.0065918  0.02197266 0.0402832  0.0625     0.08514404 0.12109375\n",
      " 0.2109375  0.53640747]\n",
      "[0.015625   0.0625     0.125      0.1640625  0.29296875 0.4375\n",
      " 0.625      0.84375    1.         1.25       1.5        2.11486816\n",
      " 3.08789062 4.25       6.         8.25      ]\n",
      "Processing embedding layer: transformer.wte\n",
      "x_with_max_frequency for transformer.wte: -4.26\n",
      "Quantized embedding layer transformer.wte with scale factor: 1.91e+01\n",
      "Processing embedding layer: transformer.wpe\n",
      "x_with_max_frequency for transformer.wpe: -7.87\n",
      "Quantized embedding layer transformer.wpe with scale factor: 2.33e+02\n",
      "Processing layer: transformer.h.0.attn.c_attn\n",
      "x_with_max_frequency for transformer.h.0.attn.c_attn: -4.47\n",
      "Quantized layer transformer.h.0.attn.c_attn with scale factor: 2.22e+01\n",
      "Processing layer: transformer.h.0.attn.c_proj\n",
      "x_with_max_frequency for transformer.h.0.attn.c_proj: -5.24\n",
      "Quantized layer transformer.h.0.attn.c_proj with scale factor: 3.78e+01\n",
      "Processing layer: transformer.h.0.mlp.c_fc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31802/440090267.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  quantized_weights = posit_quantize(torch.tensor(module.weight.data, dtype=torch.float32), nsize=5, es=1, scale=scale)\n",
      "/tmp/ipykernel_31802/440090267.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  quantized_weights = posit_quantize(torch.tensor(module.weight.data, dtype=torch.float32), nsize=5, es=1, scale=scale)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_with_max_frequency for transformer.h.0.mlp.c_fc: -4.50\n",
      "Quantized layer transformer.h.0.mlp.c_fc with scale factor: 2.27e+01\n",
      "Processing layer: transformer.h.0.mlp.c_proj\n",
      "x_with_max_frequency for transformer.h.0.mlp.c_proj: -5.06\n",
      "Quantized layer transformer.h.0.mlp.c_proj with scale factor: 3.34e+01\n",
      "Processing layer: transformer.h.1.attn.c_attn\n",
      "x_with_max_frequency for transformer.h.1.attn.c_attn: -4.60\n",
      "Quantized layer transformer.h.1.attn.c_attn with scale factor: 2.42e+01\n",
      "Processing layer: transformer.h.1.attn.c_proj\n",
      "x_with_max_frequency for transformer.h.1.attn.c_proj: -5.01\n",
      "Quantized layer transformer.h.1.attn.c_proj with scale factor: 3.22e+01\n",
      "Processing layer: transformer.h.1.mlp.c_fc\n",
      "x_with_max_frequency for transformer.h.1.mlp.c_fc: -4.44\n",
      "Quantized layer transformer.h.1.mlp.c_fc with scale factor: 2.17e+01\n",
      "Processing layer: transformer.h.1.mlp.c_proj\n",
      "x_with_max_frequency for transformer.h.1.mlp.c_proj: -4.69\n",
      "Quantized layer transformer.h.1.mlp.c_proj with scale factor: 2.58e+01\n",
      "Processing layer: transformer.h.2.attn.c_attn\n",
      "x_with_max_frequency for transformer.h.2.attn.c_attn: -4.82\n",
      "Quantized layer transformer.h.2.attn.c_attn with scale factor: 2.82e+01\n",
      "Processing layer: transformer.h.2.attn.c_proj\n",
      "x_with_max_frequency for transformer.h.2.attn.c_proj: -5.04\n",
      "Quantized layer transformer.h.2.attn.c_proj with scale factor: 3.29e+01\n",
      "Processing layer: transformer.h.2.mlp.c_fc\n",
      "x_with_max_frequency for transformer.h.2.mlp.c_fc: -4.33\n",
      "Quantized layer transformer.h.2.mlp.c_fc with scale factor: 2.01e+01\n",
      "Processing layer: transformer.h.2.mlp.c_proj\n",
      "x_with_max_frequency for transformer.h.2.mlp.c_proj: -4.62\n",
      "Quantized layer transformer.h.2.mlp.c_proj with scale factor: 2.46e+01\n",
      "Processing layer: transformer.h.3.attn.c_attn\n",
      "x_with_max_frequency for transformer.h.3.attn.c_attn: -4.97\n",
      "Quantized layer transformer.h.3.attn.c_attn with scale factor: 3.12e+01\n",
      "Processing layer: transformer.h.3.attn.c_proj\n",
      "x_with_max_frequency for transformer.h.3.attn.c_proj: -4.85\n",
      "Quantized layer transformer.h.3.attn.c_proj with scale factor: 2.88e+01\n",
      "Processing layer: transformer.h.3.mlp.c_fc\n",
      "x_with_max_frequency for transformer.h.3.mlp.c_fc: -4.42\n",
      "Quantized layer transformer.h.3.mlp.c_fc with scale factor: 2.14e+01\n",
      "Processing layer: transformer.h.3.mlp.c_proj\n",
      "x_with_max_frequency for transformer.h.3.mlp.c_proj: -4.57\n",
      "Quantized layer transformer.h.3.mlp.c_proj with scale factor: 2.37e+01\n",
      "Processing layer: transformer.h.4.attn.c_attn\n",
      "x_with_max_frequency for transformer.h.4.attn.c_attn: -4.76\n",
      "Quantized layer transformer.h.4.attn.c_attn with scale factor: 2.71e+01\n",
      "Processing layer: transformer.h.4.attn.c_proj\n",
      "x_with_max_frequency for transformer.h.4.attn.c_proj: -4.89\n",
      "Quantized layer transformer.h.4.attn.c_proj with scale factor: 2.96e+01\n",
      "Processing layer: transformer.h.4.mlp.c_fc\n",
      "x_with_max_frequency for transformer.h.4.mlp.c_fc: -4.46\n",
      "Quantized layer transformer.h.4.mlp.c_fc with scale factor: 2.20e+01\n",
      "Processing layer: transformer.h.4.mlp.c_proj\n",
      "x_with_max_frequency for transformer.h.4.mlp.c_proj: -4.57\n",
      "Quantized layer transformer.h.4.mlp.c_proj with scale factor: 2.38e+01\n",
      "Processing layer: transformer.h.5.attn.c_attn\n",
      "x_with_max_frequency for transformer.h.5.attn.c_attn: -4.71\n",
      "Quantized layer transformer.h.5.attn.c_attn with scale factor: 2.62e+01\n",
      "Processing layer: transformer.h.5.attn.c_proj\n",
      "x_with_max_frequency for transformer.h.5.attn.c_proj: -4.91\n",
      "Quantized layer transformer.h.5.attn.c_proj with scale factor: 3.01e+01\n",
      "Processing layer: transformer.h.5.mlp.c_fc\n",
      "x_with_max_frequency for transformer.h.5.mlp.c_fc: -4.50\n",
      "Quantized layer transformer.h.5.mlp.c_fc with scale factor: 2.26e+01\n",
      "Processing layer: transformer.h.5.mlp.c_proj\n",
      "x_with_max_frequency for transformer.h.5.mlp.c_proj: -4.51\n",
      "Quantized layer transformer.h.5.mlp.c_proj with scale factor: 2.28e+01\n",
      "Processing layer: transformer.h.6.attn.c_attn\n",
      "x_with_max_frequency for transformer.h.6.attn.c_attn: -4.67\n",
      "Quantized layer transformer.h.6.attn.c_attn with scale factor: 2.55e+01\n",
      "Processing layer: transformer.h.6.attn.c_proj\n",
      "x_with_max_frequency for transformer.h.6.attn.c_proj: -4.84\n",
      "Quantized layer transformer.h.6.attn.c_proj with scale factor: 2.86e+01\n",
      "Processing layer: transformer.h.6.mlp.c_fc\n",
      "x_with_max_frequency for transformer.h.6.mlp.c_fc: -4.37\n",
      "Quantized layer transformer.h.6.mlp.c_fc with scale factor: 2.06e+01\n",
      "Processing layer: transformer.h.6.mlp.c_proj\n",
      "x_with_max_frequency for transformer.h.6.mlp.c_proj: -4.53\n",
      "Quantized layer transformer.h.6.mlp.c_proj with scale factor: 2.32e+01\n",
      "Processing layer: transformer.h.7.attn.c_attn\n",
      "x_with_max_frequency for transformer.h.7.attn.c_attn: -4.56\n",
      "Quantized layer transformer.h.7.attn.c_attn with scale factor: 2.36e+01\n",
      "Processing layer: transformer.h.7.attn.c_proj\n",
      "x_with_max_frequency for transformer.h.7.attn.c_proj: -4.84\n",
      "Quantized layer transformer.h.7.attn.c_proj with scale factor: 2.87e+01\n",
      "Processing layer: transformer.h.7.mlp.c_fc\n",
      "x_with_max_frequency for transformer.h.7.mlp.c_fc: -4.48\n",
      "Quantized layer transformer.h.7.mlp.c_fc with scale factor: 2.24e+01\n",
      "Processing layer: transformer.h.7.mlp.c_proj\n",
      "x_with_max_frequency for transformer.h.7.mlp.c_proj: -4.60\n",
      "Quantized layer transformer.h.7.mlp.c_proj with scale factor: 2.43e+01\n",
      "Processing layer: transformer.h.8.attn.c_attn\n",
      "x_with_max_frequency for transformer.h.8.attn.c_attn: -4.70\n",
      "Quantized layer transformer.h.8.attn.c_attn with scale factor: 2.60e+01\n",
      "Processing layer: transformer.h.8.attn.c_proj\n",
      "x_with_max_frequency for transformer.h.8.attn.c_proj: -4.74\n",
      "Quantized layer transformer.h.8.attn.c_proj with scale factor: 2.67e+01\n",
      "Processing layer: transformer.h.8.mlp.c_fc\n",
      "x_with_max_frequency for transformer.h.8.mlp.c_fc: -4.37\n",
      "Quantized layer transformer.h.8.mlp.c_fc with scale factor: 2.06e+01\n",
      "Processing layer: transformer.h.8.mlp.c_proj\n",
      "x_with_max_frequency for transformer.h.8.mlp.c_proj: -4.62\n",
      "Quantized layer transformer.h.8.mlp.c_proj with scale factor: 2.46e+01\n",
      "Processing layer: transformer.h.9.attn.c_attn\n",
      "x_with_max_frequency for transformer.h.9.attn.c_attn: -4.52\n",
      "Quantized layer transformer.h.9.attn.c_attn with scale factor: 2.30e+01\n",
      "Processing layer: transformer.h.9.attn.c_proj\n",
      "x_with_max_frequency for transformer.h.9.attn.c_proj: -4.70\n",
      "Quantized layer transformer.h.9.attn.c_proj with scale factor: 2.61e+01\n",
      "Processing layer: transformer.h.9.mlp.c_fc\n",
      "x_with_max_frequency for transformer.h.9.mlp.c_fc: -4.43\n",
      "Quantized layer transformer.h.9.mlp.c_fc with scale factor: 2.16e+01\n",
      "Processing layer: transformer.h.9.mlp.c_proj\n",
      "x_with_max_frequency for transformer.h.9.mlp.c_proj: -4.74\n",
      "Quantized layer transformer.h.9.mlp.c_proj with scale factor: 2.67e+01\n",
      "Processing layer: transformer.h.10.attn.c_attn\n",
      "x_with_max_frequency for transformer.h.10.attn.c_attn: -4.41\n",
      "Quantized layer transformer.h.10.attn.c_attn with scale factor: 2.13e+01\n",
      "Processing layer: transformer.h.10.attn.c_proj\n",
      "x_with_max_frequency for transformer.h.10.attn.c_proj: -4.84\n",
      "Quantized layer transformer.h.10.attn.c_proj with scale factor: 2.87e+01\n",
      "Processing layer: transformer.h.10.mlp.c_fc\n",
      "x_with_max_frequency for transformer.h.10.mlp.c_fc: -4.45\n",
      "Quantized layer transformer.h.10.mlp.c_fc with scale factor: 2.19e+01\n",
      "Processing layer: transformer.h.10.mlp.c_proj\n",
      "x_with_max_frequency for transformer.h.10.mlp.c_proj: -4.64\n",
      "Quantized layer transformer.h.10.mlp.c_proj with scale factor: 2.49e+01\n",
      "Processing layer: transformer.h.11.attn.c_attn\n",
      "x_with_max_frequency for transformer.h.11.attn.c_attn: -4.40\n",
      "Quantized layer transformer.h.11.attn.c_attn with scale factor: 2.10e+01\n",
      "Processing layer: transformer.h.11.attn.c_proj\n",
      "x_with_max_frequency for transformer.h.11.attn.c_proj: -4.64\n",
      "Quantized layer transformer.h.11.attn.c_proj with scale factor: 2.50e+01\n",
      "Processing layer: transformer.h.11.mlp.c_fc\n",
      "x_with_max_frequency for transformer.h.11.mlp.c_fc: -4.37\n",
      "Quantized layer transformer.h.11.mlp.c_fc with scale factor: 2.07e+01\n",
      "Processing layer: transformer.h.11.mlp.c_proj\n",
      "x_with_max_frequency for transformer.h.11.mlp.c_proj: -4.53\n",
      "Quantized layer transformer.h.11.mlp.c_proj with scale factor: 2.31e+01\n",
      "Processing layer: transformer.h.12.attn.c_attn\n",
      "x_with_max_frequency for transformer.h.12.attn.c_attn: -4.51\n",
      "Quantized layer transformer.h.12.attn.c_attn with scale factor: 2.27e+01\n",
      "Processing layer: transformer.h.12.attn.c_proj\n",
      "x_with_max_frequency for transformer.h.12.attn.c_proj: -4.75\n",
      "Quantized layer transformer.h.12.attn.c_proj with scale factor: 2.70e+01\n",
      "Processing layer: transformer.h.12.mlp.c_fc\n",
      "x_with_max_frequency for transformer.h.12.mlp.c_fc: -4.49\n",
      "Quantized layer transformer.h.12.mlp.c_fc with scale factor: 2.25e+01\n",
      "Processing layer: transformer.h.12.mlp.c_proj\n",
      "x_with_max_frequency for transformer.h.12.mlp.c_proj: -4.53\n",
      "Quantized layer transformer.h.12.mlp.c_proj with scale factor: 2.31e+01\n",
      "Processing layer: transformer.h.13.attn.c_attn\n",
      "x_with_max_frequency for transformer.h.13.attn.c_attn: -4.49\n",
      "Quantized layer transformer.h.13.attn.c_attn with scale factor: 2.25e+01\n",
      "Processing layer: transformer.h.13.attn.c_proj\n",
      "x_with_max_frequency for transformer.h.13.attn.c_proj: -4.75\n",
      "Quantized layer transformer.h.13.attn.c_proj with scale factor: 2.69e+01\n",
      "Processing layer: transformer.h.13.mlp.c_fc\n",
      "x_with_max_frequency for transformer.h.13.mlp.c_fc: -4.36\n",
      "Quantized layer transformer.h.13.mlp.c_fc with scale factor: 2.06e+01\n",
      "Processing layer: transformer.h.13.mlp.c_proj\n",
      "x_with_max_frequency for transformer.h.13.mlp.c_proj: -4.57\n",
      "Quantized layer transformer.h.13.mlp.c_proj with scale factor: 2.38e+01\n",
      "Processing layer: transformer.h.14.attn.c_attn\n",
      "x_with_max_frequency for transformer.h.14.attn.c_attn: -4.51\n",
      "Quantized layer transformer.h.14.attn.c_attn with scale factor: 2.28e+01\n",
      "Processing layer: transformer.h.14.attn.c_proj\n",
      "x_with_max_frequency for transformer.h.14.attn.c_proj: -4.71\n",
      "Quantized layer transformer.h.14.attn.c_proj with scale factor: 2.61e+01\n",
      "Processing layer: transformer.h.14.mlp.c_fc\n",
      "x_with_max_frequency for transformer.h.14.mlp.c_fc: -4.33\n",
      "Quantized layer transformer.h.14.mlp.c_fc with scale factor: 2.01e+01\n",
      "Processing layer: transformer.h.14.mlp.c_proj\n",
      "x_with_max_frequency for transformer.h.14.mlp.c_proj: -4.76\n",
      "Quantized layer transformer.h.14.mlp.c_proj with scale factor: 2.70e+01\n",
      "Processing layer: transformer.h.15.attn.c_attn\n",
      "x_with_max_frequency for transformer.h.15.attn.c_attn: -4.36\n",
      "Quantized layer transformer.h.15.attn.c_attn with scale factor: 2.05e+01\n",
      "Processing layer: transformer.h.15.attn.c_proj\n",
      "x_with_max_frequency for transformer.h.15.attn.c_proj: -4.66\n",
      "Quantized layer transformer.h.15.attn.c_proj with scale factor: 2.53e+01\n",
      "Processing layer: transformer.h.15.mlp.c_fc\n",
      "x_with_max_frequency for transformer.h.15.mlp.c_fc: -4.32\n",
      "Quantized layer transformer.h.15.mlp.c_fc with scale factor: 1.99e+01\n",
      "Processing layer: transformer.h.15.mlp.c_proj\n",
      "x_with_max_frequency for transformer.h.15.mlp.c_proj: -4.75\n",
      "Quantized layer transformer.h.15.mlp.c_proj with scale factor: 2.69e+01\n",
      "Processing layer: transformer.h.16.attn.c_attn\n",
      "x_with_max_frequency for transformer.h.16.attn.c_attn: -4.53\n",
      "Quantized layer transformer.h.16.attn.c_attn with scale factor: 2.31e+01\n",
      "Processing layer: transformer.h.16.attn.c_proj\n",
      "x_with_max_frequency for transformer.h.16.attn.c_proj: -4.53\n",
      "Quantized layer transformer.h.16.attn.c_proj with scale factor: 2.31e+01\n",
      "Processing layer: transformer.h.16.mlp.c_fc\n",
      "x_with_max_frequency for transformer.h.16.mlp.c_fc: -4.34\n",
      "Quantized layer transformer.h.16.mlp.c_fc with scale factor: 2.03e+01\n",
      "Processing layer: transformer.h.16.mlp.c_proj\n",
      "x_with_max_frequency for transformer.h.16.mlp.c_proj: -4.55\n",
      "Quantized layer transformer.h.16.mlp.c_proj with scale factor: 2.34e+01\n",
      "Processing layer: transformer.h.17.attn.c_attn\n",
      "x_with_max_frequency for transformer.h.17.attn.c_attn: -4.40\n",
      "Quantized layer transformer.h.17.attn.c_attn with scale factor: 2.11e+01\n",
      "Processing layer: transformer.h.17.attn.c_proj\n",
      "x_with_max_frequency for transformer.h.17.attn.c_proj: -4.51\n",
      "Quantized layer transformer.h.17.attn.c_proj with scale factor: 2.28e+01\n",
      "Processing layer: transformer.h.17.mlp.c_fc\n",
      "x_with_max_frequency for transformer.h.17.mlp.c_fc: -4.56\n",
      "Quantized layer transformer.h.17.mlp.c_fc with scale factor: 2.35e+01\n",
      "Processing layer: transformer.h.17.mlp.c_proj\n",
      "x_with_max_frequency for transformer.h.17.mlp.c_proj: -4.51\n",
      "Quantized layer transformer.h.17.mlp.c_proj with scale factor: 2.27e+01\n",
      "Processing layer: transformer.h.18.attn.c_attn\n",
      "x_with_max_frequency for transformer.h.18.attn.c_attn: -4.30\n",
      "Quantized layer transformer.h.18.attn.c_attn with scale factor: 1.97e+01\n",
      "Processing layer: transformer.h.18.attn.c_proj\n",
      "x_with_max_frequency for transformer.h.18.attn.c_proj: -4.27\n",
      "Quantized layer transformer.h.18.attn.c_proj with scale factor: 1.93e+01\n",
      "Processing layer: transformer.h.18.mlp.c_fc\n",
      "x_with_max_frequency for transformer.h.18.mlp.c_fc: -4.53\n",
      "Quantized layer transformer.h.18.mlp.c_fc with scale factor: 2.30e+01\n",
      "Processing layer: transformer.h.18.mlp.c_proj\n",
      "x_with_max_frequency for transformer.h.18.mlp.c_proj: -4.48\n",
      "Quantized layer transformer.h.18.mlp.c_proj with scale factor: 2.23e+01\n",
      "Processing layer: transformer.h.19.attn.c_attn\n",
      "x_with_max_frequency for transformer.h.19.attn.c_attn: -4.39\n",
      "Quantized layer transformer.h.19.attn.c_attn with scale factor: 2.10e+01\n",
      "Processing layer: transformer.h.19.attn.c_proj\n",
      "x_with_max_frequency for transformer.h.19.attn.c_proj: -4.35\n",
      "Quantized layer transformer.h.19.attn.c_proj with scale factor: 2.04e+01\n",
      "Processing layer: transformer.h.19.mlp.c_fc\n",
      "x_with_max_frequency for transformer.h.19.mlp.c_fc: -4.59\n",
      "Quantized layer transformer.h.19.mlp.c_fc with scale factor: 2.40e+01\n",
      "Processing layer: transformer.h.19.mlp.c_proj\n",
      "x_with_max_frequency for transformer.h.19.mlp.c_proj: -4.53\n",
      "Quantized layer transformer.h.19.mlp.c_proj with scale factor: 2.30e+01\n",
      "Processing layer: transformer.h.20.attn.c_attn\n",
      "x_with_max_frequency for transformer.h.20.attn.c_attn: -4.47\n",
      "Quantized layer transformer.h.20.attn.c_attn with scale factor: 2.22e+01\n",
      "Processing layer: transformer.h.20.attn.c_proj\n",
      "x_with_max_frequency for transformer.h.20.attn.c_proj: -4.40\n",
      "Quantized layer transformer.h.20.attn.c_proj with scale factor: 2.11e+01\n",
      "Processing layer: transformer.h.20.mlp.c_fc\n",
      "x_with_max_frequency for transformer.h.20.mlp.c_fc: -4.35\n",
      "Quantized layer transformer.h.20.mlp.c_fc with scale factor: 2.04e+01\n",
      "Processing layer: transformer.h.20.mlp.c_proj\n",
      "x_with_max_frequency for transformer.h.20.mlp.c_proj: -4.49\n",
      "Quantized layer transformer.h.20.mlp.c_proj with scale factor: 2.25e+01\n",
      "Processing layer: transformer.h.21.attn.c_attn\n",
      "x_with_max_frequency for transformer.h.21.attn.c_attn: -4.33\n",
      "Quantized layer transformer.h.21.attn.c_attn with scale factor: 2.00e+01\n",
      "Processing layer: transformer.h.21.attn.c_proj\n",
      "x_with_max_frequency for transformer.h.21.attn.c_proj: -4.49\n",
      "Quantized layer transformer.h.21.attn.c_proj with scale factor: 2.25e+01\n",
      "Processing layer: transformer.h.21.mlp.c_fc\n",
      "x_with_max_frequency for transformer.h.21.mlp.c_fc: -4.54\n",
      "Quantized layer transformer.h.21.mlp.c_fc with scale factor: 2.33e+01\n",
      "Processing layer: transformer.h.21.mlp.c_proj\n",
      "x_with_max_frequency for transformer.h.21.mlp.c_proj: -4.47\n",
      "Quantized layer transformer.h.21.mlp.c_proj with scale factor: 2.21e+01\n",
      "Processing layer: transformer.h.22.attn.c_attn\n",
      "x_with_max_frequency for transformer.h.22.attn.c_attn: -4.34\n",
      "Quantized layer transformer.h.22.attn.c_attn with scale factor: 2.03e+01\n",
      "Processing layer: transformer.h.22.attn.c_proj\n",
      "x_with_max_frequency for transformer.h.22.attn.c_proj: -4.35\n",
      "Quantized layer transformer.h.22.attn.c_proj with scale factor: 2.04e+01\n",
      "Processing layer: transformer.h.22.mlp.c_fc\n",
      "x_with_max_frequency for transformer.h.22.mlp.c_fc: -4.56\n",
      "Quantized layer transformer.h.22.mlp.c_fc with scale factor: 2.35e+01\n",
      "Processing layer: transformer.h.22.mlp.c_proj\n",
      "x_with_max_frequency for transformer.h.22.mlp.c_proj: -4.43\n",
      "Quantized layer transformer.h.22.mlp.c_proj with scale factor: 2.16e+01\n",
      "Processing layer: transformer.h.23.attn.c_attn\n",
      "x_with_max_frequency for transformer.h.23.attn.c_attn: -4.46\n",
      "Quantized layer transformer.h.23.attn.c_attn with scale factor: 2.19e+01\n",
      "Processing layer: transformer.h.23.attn.c_proj\n",
      "x_with_max_frequency for transformer.h.23.attn.c_proj: -4.48\n",
      "Quantized layer transformer.h.23.attn.c_proj with scale factor: 2.23e+01\n",
      "Processing layer: transformer.h.23.mlp.c_fc\n",
      "x_with_max_frequency for transformer.h.23.mlp.c_fc: -4.50\n",
      "Quantized layer transformer.h.23.mlp.c_fc with scale factor: 2.26e+01\n",
      "Processing layer: transformer.h.23.mlp.c_proj\n",
      "x_with_max_frequency for transformer.h.23.mlp.c_proj: -4.46\n",
      "Quantized layer transformer.h.23.mlp.c_proj with scale factor: 2.20e+01\n",
      "Processing layer: transformer.h.24.attn.c_attn\n",
      "x_with_max_frequency for transformer.h.24.attn.c_attn: -4.39\n",
      "Quantized layer transformer.h.24.attn.c_attn with scale factor: 2.10e+01\n",
      "Processing layer: transformer.h.24.attn.c_proj\n",
      "x_with_max_frequency for transformer.h.24.attn.c_proj: -4.43\n",
      "Quantized layer transformer.h.24.attn.c_proj with scale factor: 2.16e+01\n",
      "Processing layer: transformer.h.24.mlp.c_fc\n",
      "x_with_max_frequency for transformer.h.24.mlp.c_fc: -4.38\n",
      "Quantized layer transformer.h.24.mlp.c_fc with scale factor: 2.08e+01\n",
      "Processing layer: transformer.h.24.mlp.c_proj\n",
      "x_with_max_frequency for transformer.h.24.mlp.c_proj: -4.20\n",
      "Quantized layer transformer.h.24.mlp.c_proj with scale factor: 1.84e+01\n",
      "Processing layer: transformer.h.25.attn.c_attn\n",
      "x_with_max_frequency for transformer.h.25.attn.c_attn: -4.36\n",
      "Quantized layer transformer.h.25.attn.c_attn with scale factor: 2.05e+01\n",
      "Processing layer: transformer.h.25.attn.c_proj\n",
      "x_with_max_frequency for transformer.h.25.attn.c_proj: -4.31\n",
      "Quantized layer transformer.h.25.attn.c_proj with scale factor: 1.98e+01\n",
      "Processing layer: transformer.h.25.mlp.c_fc\n",
      "x_with_max_frequency for transformer.h.25.mlp.c_fc: -4.53\n",
      "Quantized layer transformer.h.25.mlp.c_fc with scale factor: 2.31e+01\n",
      "Processing layer: transformer.h.25.mlp.c_proj\n",
      "x_with_max_frequency for transformer.h.25.mlp.c_proj: -4.25\n",
      "Quantized layer transformer.h.25.mlp.c_proj with scale factor: 1.91e+01\n",
      "Processing layer: transformer.h.26.attn.c_attn\n",
      "x_with_max_frequency for transformer.h.26.attn.c_attn: -4.49\n",
      "Quantized layer transformer.h.26.attn.c_attn with scale factor: 2.25e+01\n",
      "Processing layer: transformer.h.26.attn.c_proj\n",
      "x_with_max_frequency for transformer.h.26.attn.c_proj: -4.29\n",
      "Quantized layer transformer.h.26.attn.c_proj with scale factor: 1.95e+01\n",
      "Processing layer: transformer.h.26.mlp.c_fc\n",
      "x_with_max_frequency for transformer.h.26.mlp.c_fc: -4.42\n",
      "Quantized layer transformer.h.26.mlp.c_fc with scale factor: 2.14e+01\n",
      "Processing layer: transformer.h.26.mlp.c_proj\n",
      "x_with_max_frequency for transformer.h.26.mlp.c_proj: -4.19\n",
      "Quantized layer transformer.h.26.mlp.c_proj with scale factor: 1.83e+01\n",
      "Processing layer: transformer.h.27.attn.c_attn\n",
      "x_with_max_frequency for transformer.h.27.attn.c_attn: -4.30\n",
      "Quantized layer transformer.h.27.attn.c_attn with scale factor: 1.97e+01\n",
      "Processing layer: transformer.h.27.attn.c_proj\n",
      "x_with_max_frequency for transformer.h.27.attn.c_proj: -4.42\n",
      "Quantized layer transformer.h.27.attn.c_proj with scale factor: 2.14e+01\n",
      "Processing layer: transformer.h.27.mlp.c_fc\n",
      "x_with_max_frequency for transformer.h.27.mlp.c_fc: -4.31\n",
      "Quantized layer transformer.h.27.mlp.c_fc with scale factor: 1.98e+01\n",
      "Processing layer: transformer.h.27.mlp.c_proj\n",
      "x_with_max_frequency for transformer.h.27.mlp.c_proj: -4.28\n",
      "Quantized layer transformer.h.27.mlp.c_proj with scale factor: 1.94e+01\n",
      "Processing layer: transformer.h.28.attn.c_attn\n",
      "x_with_max_frequency for transformer.h.28.attn.c_attn: -4.37\n",
      "Quantized layer transformer.h.28.attn.c_attn with scale factor: 2.07e+01\n",
      "Processing layer: transformer.h.28.attn.c_proj\n",
      "x_with_max_frequency for transformer.h.28.attn.c_proj: -4.30\n",
      "Quantized layer transformer.h.28.attn.c_proj with scale factor: 1.96e+01\n",
      "Processing layer: transformer.h.28.mlp.c_fc\n",
      "x_with_max_frequency for transformer.h.28.mlp.c_fc: -4.37\n",
      "Quantized layer transformer.h.28.mlp.c_fc with scale factor: 2.07e+01\n",
      "Processing layer: transformer.h.28.mlp.c_proj\n",
      "x_with_max_frequency for transformer.h.28.mlp.c_proj: -4.11\n",
      "Quantized layer transformer.h.28.mlp.c_proj with scale factor: 1.72e+01\n",
      "Processing layer: transformer.h.29.attn.c_attn\n",
      "x_with_max_frequency for transformer.h.29.attn.c_attn: -4.41\n",
      "Quantized layer transformer.h.29.attn.c_attn with scale factor: 2.13e+01\n",
      "Processing layer: transformer.h.29.attn.c_proj\n",
      "x_with_max_frequency for transformer.h.29.attn.c_proj: -4.24\n",
      "Quantized layer transformer.h.29.attn.c_proj with scale factor: 1.89e+01\n",
      "Processing layer: transformer.h.29.mlp.c_fc\n",
      "x_with_max_frequency for transformer.h.29.mlp.c_fc: -4.49\n",
      "Quantized layer transformer.h.29.mlp.c_fc with scale factor: 2.25e+01\n",
      "Processing layer: transformer.h.29.mlp.c_proj\n",
      "x_with_max_frequency for transformer.h.29.mlp.c_proj: -4.21\n",
      "Quantized layer transformer.h.29.mlp.c_proj with scale factor: 1.85e+01\n",
      "Processing layer: transformer.h.30.attn.c_attn\n",
      "x_with_max_frequency for transformer.h.30.attn.c_attn: -4.50\n",
      "Quantized layer transformer.h.30.attn.c_attn with scale factor: 2.26e+01\n",
      "Processing layer: transformer.h.30.attn.c_proj\n",
      "x_with_max_frequency for transformer.h.30.attn.c_proj: -4.14\n",
      "Quantized layer transformer.h.30.attn.c_proj with scale factor: 1.76e+01\n",
      "Processing layer: transformer.h.30.mlp.c_fc\n",
      "x_with_max_frequency for transformer.h.30.mlp.c_fc: -4.33\n",
      "Quantized layer transformer.h.30.mlp.c_fc with scale factor: 2.02e+01\n",
      "Processing layer: transformer.h.30.mlp.c_proj\n",
      "x_with_max_frequency for transformer.h.30.mlp.c_proj: -4.15\n",
      "Quantized layer transformer.h.30.mlp.c_proj with scale factor: 1.78e+01\n",
      "Processing layer: transformer.h.31.attn.c_attn\n",
      "x_with_max_frequency for transformer.h.31.attn.c_attn: -4.23\n",
      "Quantized layer transformer.h.31.attn.c_attn with scale factor: 1.87e+01\n",
      "Processing layer: transformer.h.31.attn.c_proj\n",
      "x_with_max_frequency for transformer.h.31.attn.c_proj: -4.15\n",
      "Quantized layer transformer.h.31.attn.c_proj with scale factor: 1.78e+01\n",
      "Processing layer: transformer.h.31.mlp.c_fc\n",
      "x_with_max_frequency for transformer.h.31.mlp.c_fc: -4.36\n",
      "Quantized layer transformer.h.31.mlp.c_fc with scale factor: 2.05e+01\n",
      "Processing layer: transformer.h.31.mlp.c_proj\n",
      "x_with_max_frequency for transformer.h.31.mlp.c_proj: -3.92\n",
      "Quantized layer transformer.h.31.mlp.c_proj with scale factor: 1.52e+01\n",
      "Processing layer: transformer.h.32.attn.c_attn\n",
      "x_with_max_frequency for transformer.h.32.attn.c_attn: -4.26\n",
      "Quantized layer transformer.h.32.attn.c_attn with scale factor: 1.91e+01\n",
      "Processing layer: transformer.h.32.attn.c_proj\n",
      "x_with_max_frequency for transformer.h.32.attn.c_proj: -4.33\n",
      "Quantized layer transformer.h.32.attn.c_proj with scale factor: 2.02e+01\n",
      "Processing layer: transformer.h.32.mlp.c_fc\n",
      "x_with_max_frequency for transformer.h.32.mlp.c_fc: -4.49\n",
      "Quantized layer transformer.h.32.mlp.c_fc with scale factor: 2.24e+01\n",
      "Processing layer: transformer.h.32.mlp.c_proj\n",
      "x_with_max_frequency for transformer.h.32.mlp.c_proj: -3.96\n",
      "Quantized layer transformer.h.32.mlp.c_proj with scale factor: 1.56e+01\n",
      "Processing layer: transformer.h.33.attn.c_attn\n",
      "x_with_max_frequency for transformer.h.33.attn.c_attn: -4.46\n",
      "Quantized layer transformer.h.33.attn.c_attn with scale factor: 2.20e+01\n",
      "Processing layer: transformer.h.33.attn.c_proj\n",
      "x_with_max_frequency for transformer.h.33.attn.c_proj: -4.34\n",
      "Quantized layer transformer.h.33.attn.c_proj with scale factor: 2.02e+01\n",
      "Processing layer: transformer.h.33.mlp.c_fc\n",
      "x_with_max_frequency for transformer.h.33.mlp.c_fc: -4.48\n",
      "Quantized layer transformer.h.33.mlp.c_fc with scale factor: 2.24e+01\n",
      "Processing layer: transformer.h.33.mlp.c_proj\n",
      "x_with_max_frequency for transformer.h.33.mlp.c_proj: -3.94\n",
      "Quantized layer transformer.h.33.mlp.c_proj with scale factor: 1.54e+01\n",
      "Processing layer: transformer.h.34.attn.c_attn\n",
      "x_with_max_frequency for transformer.h.34.attn.c_attn: -4.36\n",
      "Quantized layer transformer.h.34.attn.c_attn with scale factor: 2.05e+01\n",
      "Processing layer: transformer.h.34.attn.c_proj\n",
      "x_with_max_frequency for transformer.h.34.attn.c_proj: -4.19\n",
      "Quantized layer transformer.h.34.attn.c_proj with scale factor: 1.83e+01\n",
      "Processing layer: transformer.h.34.mlp.c_fc\n",
      "x_with_max_frequency for transformer.h.34.mlp.c_fc: -4.46\n",
      "Quantized layer transformer.h.34.mlp.c_fc with scale factor: 2.20e+01\n",
      "Processing layer: transformer.h.34.mlp.c_proj\n",
      "x_with_max_frequency for transformer.h.34.mlp.c_proj: -3.90\n",
      "Quantized layer transformer.h.34.mlp.c_proj with scale factor: 1.49e+01\n",
      "Processing layer: transformer.h.35.attn.c_attn\n",
      "x_with_max_frequency for transformer.h.35.attn.c_attn: -4.58\n",
      "Quantized layer transformer.h.35.attn.c_attn with scale factor: 2.39e+01\n",
      "Processing layer: transformer.h.35.attn.c_proj\n",
      "x_with_max_frequency for transformer.h.35.attn.c_proj: -4.36\n",
      "Quantized layer transformer.h.35.attn.c_proj with scale factor: 2.05e+01\n",
      "Processing layer: transformer.h.35.mlp.c_fc\n",
      "x_with_max_frequency for transformer.h.35.mlp.c_fc: -4.23\n",
      "Quantized layer transformer.h.35.mlp.c_fc with scale factor: 1.88e+01\n",
      "Processing layer: transformer.h.35.mlp.c_proj\n",
      "x_with_max_frequency for transformer.h.35.mlp.c_proj: -4.01\n",
      "Quantized layer transformer.h.35.mlp.c_proj with scale factor: 1.62e+01\n",
      "Processing layer: lm_head\n",
      "x_with_max_frequency for lm_head: -4.27\n",
      "Quantized layer lm_head with scale factor: 1.93e+01\n",
      "Total layers processed: 145\n",
      "MAC operation count: 772117760\n",
      "MAC operation count: 772117760\n",
      "Layer count: 145\n",
      "MAC operation count  772117760\n",
      "Layer count  145\n",
      "Machine learning is the study of rules to be applied to data, and data to be analyzed in order to reach a common objective.\n",
      "\n",
      "What you will be reading\n",
      "\n",
      "In this article you will be introduced how to create a dataset of training data\n",
      "-----------------\n",
      "Machine learning is the study of data structure and the underlying theory behind it by the experts. The most common model for a machine learning algorithm is one that learns from a data point (or the data) in different ways, which is the case of Big\n",
      "-----------------------------------\n",
      "Machine learning is the study of how a set of problems can be combined.\n",
      "\n",
      "Let's create a list that we'll use in place of the normal C# implementation of a singleton and in our real implementation a multi-terrain with 3\n",
      "-----------------------------------\n",
      "In the 19th century, the invention of the steam gun and its use in World War II catapulted gunpowder technology into common practice. The US Navy adopted the gunpowder system as their primary munition, creating a system that would see the development\n",
      "-----------------\n",
      "In the 19th century, the invention of an early, revolutionary device for taking pictures made it impossible for most people to continue to print. Until these and other advances in printing made the need for personal photography obsolete in the first, it was impossible for\n",
      "-----------------------------------\n",
      "In the 19th century, the invention of chemical machines meant that the process of making, storing, shipping, and consuming food was an integral and fundamental part of most of the economic transactions that followed. Most people were working with a metal (or wood\n",
      "-----------------------------------\n",
      "A robot was created for the production line of the German manufacturer Daims-Amt e.V. The \"coupe\" (a type of robot which allows for easy access to its operator) contains a microcomputer, a high-\n",
      "-----------------\n",
      "A robot was created to simulate a robotic attack by firing an anti-aircraft rocket. In this case, the R-6 and the R-7 were deployed as a front-line in the war, while the B-52 was deployed as\n",
      "-----------------------------------\n",
      "A robot was created as a weapon of war and the new body is completely destroyed when a new human robot is built by its creator as a punishment of his own creation.\n",
      "\n",
      "Manga:\n",
      "\n",
      "1.1.8 The main protagonist who\n",
      "-----------------------------------\n",
      "One day I will go to the same place as the other guy had and I will see him in a mirror and he will see me and all I will say to him for the first time was \"your ass\". He will say \"how dare you\n",
      "-----------------\n",
      "One day I will not be able to find the place that I am, and that's when your friend will come. He will help you. I will wait for a moment.\" It was in this moment, when his companion was talking to himself and\n",
      "-----------------------------------\n",
      "One day I will be a master of the world and one day, I will make a dream come true.\n",
      "\n",
      "Mister Cilvia,\n",
      "\n",
      "P.S. If you were to see a world without men, a world of women\n",
      "-----------------------------------\n",
      "\n",
      " FP32 ref : \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import set_seed\n",
    "from qtorch_plus.quant import posit_quantize, float_quantize, configurable_table_quantize\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "\n",
    "\n",
    "#final with 3 layers skipped :\n",
    "full_table = np.array([6.59179688e-03, 2.19726562e-02, 4.02832031e-02, 6.25000000e-02,\n",
    "                8.51440430e-02, 2.10937500e-01, 1.21093750e-01, 5.36407471e-01,\n",
    "                1.56250000e-02, 6.25000000e-02, 1.25000000e-01, 1.64062500e-01,\n",
    "                2.92968750e-01, 6.25000000e-01, 8.43750000e-01, 4.37500000e-01,\n",
    "                1.00000000e+00, 2.11486816e+00, 1.50000000e+00, 1.25000000e+00,\n",
    "                4.25000000e+00, 3.08789062e+00, 8.25000000e+00, 6.00000000e+00])\n",
    "\n",
    "weight_table = full_table[:8]\n",
    "act_table = full_table[8:]\n",
    "weight_table = np.sort(weight_table)\n",
    "act_table = np.sort(act_table)\n",
    "print (len(weight_table))\n",
    "print (weight_table)\n",
    "print (act_table)\n",
    "\n",
    "def linear_weight(input):\n",
    "    # return input\n",
    "    # return configurable_table_quantize(input, torch.tensor(weight_table,dtype = torch.float), scale= 1.0)\n",
    "    return posit_quantize(input,nsize=8, es=1, scale = 1)\n",
    "    # return float_quantize(input,exp=4, man=3, rounding=\"nearest\")\n",
    "\n",
    "def linear_activation(input):\n",
    "    # return input\n",
    "    # return configurable_table_quantize(input,torch.tensor(act_table, dtype=torch.float), scale= 1.0)\n",
    "    return posit_quantize(input,nsize=6, es=0, scale = 1)\n",
    "    # return float_quantize(input,exp=4, man=3, rounding=\"nearest\")\n",
    "\n",
    "def forward_pre_hook_linear(m, input):\n",
    "    return (linear_activation(input[0]),)\n",
    "\n",
    "# model = model.to(device)\n",
    "# layer_count = 0\n",
    "# linear_layer_count = 0\n",
    "# op_count = 0\n",
    "# epsilon = 1e-12  # To avoid log(0)\n",
    "# import transformers.modeling_utils as  modeling_utils\n",
    "\n",
    "# for name, module in model.named_modules():\n",
    "#     # Check if the module is quantizable\n",
    "#     if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear) or isinstance(module, modeling_utils.Conv1D):\n",
    "#         layer_count += 1\n",
    "        \n",
    "#         # Apply different quantization configurations for specific layers\n",
    "#         if name == \"transformer.h.0.attn.c_attn\":\n",
    "#             print(f\"Quantizing layer {name} with Posit<8,1>\")\n",
    "#             module.weight.data = posit_quantize(module.weight.data, nsize=8, es=1)\n",
    "#         elif name == \"transformer.h.1.attn.c_attn\":\n",
    "#             print(f\"Quantizing layer {name} with Posit<16,2>\")\n",
    "#             module.weight.data = posit_quantize(module.weight.data, nsize=16, es=2)\n",
    "#         elif name == \"transformer.h.2.attn.c_attn\":\n",
    "#             print(f\"Quantizing layer {name} with Float<4,3>\")\n",
    "#             module.weight.data = float_quantize(module.weight.data, exp=4, man=3, rounding=\"nearest\")\n",
    "#         else:\n",
    "#             # Default quantization\n",
    "#             print(f\"Quantizing layer {name} with default Posit<8,1>\")\n",
    "#             module.weight.data = posit_quantize(module.weight.data, nsize=8, es=1)\n",
    "\n",
    "#         # Register forward pre-hook for activations\n",
    "#         module.register_forward_pre_hook(forward_pre_hook_linear)\n",
    "\n",
    "#         # Count operations\n",
    "#         if isinstance(module, modeling_utils.Conv1D):\n",
    "#             op_count += module.weight.shape[0] * module.weight.shape[1]\n",
    "#         else:\n",
    "#             op_count += module.in_features * module.out_features\n",
    "    \n",
    "#     # Handle embeddings separately\n",
    "#     elif isinstance(module, nn.Embedding):\n",
    "#         print(f\"Quantizing embedding layer {name} with Posit<8,1>\")\n",
    "#         module.weight.data = posit_quantize(module.weight.data, nsize=8, es=1)\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "layer_count = 0\n",
    "op_count = 0\n",
    "epsilon = 1e-12  # To avoid log(0)\n",
    "import transformers.modeling_utils as modeling_utils\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    # Check if the module is quantizable\n",
    "    if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear) or isinstance(module, modeling_utils.Conv1D):\n",
    "        layer_count += 1\n",
    "        print(f\"Processing layer: {name}\")\n",
    "\n",
    "        # Flatten the weights and compute log2 scale\n",
    "        weights_flattened = module.weight.data.cpu().numpy().flatten()\n",
    "        log2_weights = np.log2(np.abs(weights_flattened) + epsilon)\n",
    "        counts, bins = np.histogram(log2_weights, bins=100)\n",
    "        max_bin_index = np.argmax(counts)\n",
    "        x_with_max_frequency = (bins[max_bin_index] + bins[max_bin_index + 1]) / 2  # Bin center\n",
    "        print(f\"x_with_max_frequency for {name}: {x_with_max_frequency:.2f}\")\n",
    "        scale = 2 ** (-x_with_max_frequency)\n",
    "        quantized_weights = posit_quantize(torch.tensor(module.weight.data, dtype=torch.float32), nsize=6, es=0, scale=scale)\n",
    "        module.weight.data = quantized_weights\n",
    "\n",
    "        print(f\"Quantized layer {name} with scale factor: {scale:.2e}\")\n",
    "        module.register_forward_pre_hook(forward_pre_hook_linear)\n",
    "\n",
    "\n",
    "        # Count operations\n",
    "        if isinstance(module, modeling_utils.Conv1D):\n",
    "            op_count += module.weight.shape[0] * module.weight.shape[1]\n",
    "        else:\n",
    "            op_count += module.in_features * module.out_features\n",
    "\n",
    "    # Handle embeddings separately\n",
    "    elif isinstance(module, nn.Embedding):\n",
    "        print(f\"Processing embedding layer: {name}\")\n",
    "\n",
    "        # Flatten the weights and compute log2 scale\n",
    "        weights_flattened = module.weight.data.cpu().numpy().flatten()\n",
    "        log2_weights = np.log2(np.abs(weights_flattened) + epsilon)\n",
    "\n",
    "        # Compute histogram and find the bin with the maximum frequency\n",
    "        counts, bins = np.histogram(log2_weights, bins=100)\n",
    "        max_bin_index = np.argmax(counts)\n",
    "        x_with_max_frequency = (bins[max_bin_index] + bins[max_bin_index + 1]) / 2  # Bin center\n",
    "        print(f\"x_with_max_frequency for {name}: {x_with_max_frequency:.2f}\")\n",
    "\n",
    "        # Apply quantization with scale based on x_with_max_frequency\n",
    "        scale = 2 ** (-x_with_max_frequency)\n",
    "        quantized_weights = posit_quantize(torch.tensor(module.weight.data, dtype=torch.float32), nsize=6, es=0, scale=scale)\n",
    "        module.weight.data = quantized_weights\n",
    "\n",
    "        print(f\"Quantized embedding layer {name} with scale factor: {scale:.2e}\")\n",
    "\n",
    "print(\"Total layers processed:\", layer_count)\n",
    "print(\"MAC operation count:\", op_count)\n",
    "\n",
    "print(\"MAC operation count:\", op_count)\n",
    "print(\"Layer count:\", layer_count)\n",
    "\n",
    "print (\"MAC operation count \", op_count)\n",
    "print (\"Layer count \", layer_count)\n",
    "\n",
    "\n",
    "# from nlp import load_dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "test = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
    "encodings = tokenizer('\\n\\n'.join(test['text']), return_tensors='pt')\n",
    "#model = model.to(device)\n",
    "def generate_text(model_new):\n",
    "  text_generation = pipeline(\"text-generation\", model=model_new, tokenizer=tokenizer, device = 0)\n",
    "  #set_seed(42)\n",
    "  prefix_texts = [\"Machine learning is the study of\",\n",
    "                    \"In the 19th century, the invention\",\n",
    "                    \"A robot was created\",\n",
    "                    \"One day I will\"\n",
    "                  ]\n",
    "  for prefix_text in prefix_texts:\n",
    "    #generated_text= text_generation(prefix_text, max_length=50, do_sample=False )[0]\n",
    "    set_seed(42)\n",
    "    generated_text= text_generation(prefix_text, max_length=50, num_return_sequences=3)\n",
    "    print(generated_text[0]['generated_text'])\n",
    "    print (\"-----------------\")\n",
    "    print(generated_text[1]['generated_text'])\n",
    "    print (\"-----------------------------------\")\n",
    "    print(generated_text[2]['generated_text'])\n",
    "    print (\"-----------------------------------\")\n",
    "generate_text(model)\n",
    "\n",
    "print (\"\\n FP32 ref : \\n\")\n",
    "# model = GPT2LMHeadModel.from_pretrained(model_id).to(device)\n",
    "# generate_text(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size: 287644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 281/281 [01:04<00:00,  4.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 25.837797164916992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "max_length = model.config.n_positions\n",
    "stride = 1024\n",
    "\n",
    "lls = []\n",
    "input_size = encodings.input_ids.size(1)\n",
    "print(\"Input size:\", input_size)\n",
    "\n",
    "for i in tqdm(range(0, input_size, stride)):\n",
    "    begin_loc = max(i + stride - max_length, 0)\n",
    "    end_loc = min(i + stride, input_size)\n",
    "    trg_len = end_loc - i\n",
    "\n",
    "    input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "    target_ids = input_ids.clone()\n",
    "    target_ids[:, :-trg_len] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "        log_likelihood = outputs[0] * trg_len\n",
    "        # print(f\"Log Likelihood for step {i}: {log_likelihood}\")\n",
    "\n",
    "    lls.append(log_likelihood)\n",
    "\n",
    "if lls:  # Ensure lls is not empty\n",
    "    ppl = torch.exp(torch.stack(lls).sum() / input_size)\n",
    "    print(\"Perplexity:\", ppl.item())\n",
    "else:\n",
    "    print(\"No log likelihoods calculated.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "posit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
