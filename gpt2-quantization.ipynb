{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying INT8 dynamic quantization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original GPT-2 Generated text: University of Virginia is a top-ranking public school in the U.S., where  Â it is ranked No. 1 in the country for undergraduate education and No. 2 in the nation for graduate education. It is also the only U.S. public school that has a public enrollment rate of 2.6 percent.\n",
      "In 2007, the university announced that it would open its doors to students in the fall of 2010.\n",
      "The university will offer a three-year, $12,\n",
      "Quantized GPT-2 Generated text: University of Virginia is a top-ranking public school in the U.S., where  amusab is a way to ensure the two-year, - in-the-back-school-s-\" of school.\n",
      "(Citation and sponsorship given to, and for use in, \"K-9 \" and \"Carry On\"\n",
      "t and the T.A. and, intervening, the, the R&A, the. (-, the. \"The and\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load the pre-trained GPT-2 model from Hugging Face\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Move the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Quantization function for dynamic INT8 quantization\n",
    "def quantize_model(model, precision=8):\n",
    "    # Perform dynamic quantization only on linear layers\n",
    "    if precision == 8:\n",
    "        print(f\"Applying INT{precision} dynamic quantization...\")\n",
    "        model_quantized = torch.quantization.quantize_dynamic(\n",
    "            model,  # Model to be quantized\n",
    "            {torch.nn.Linear},  # Layers to quantize\n",
    "            dtype=torch.qint8  # Quantize to INT8\n",
    "        )\n",
    "    return model_quantized\n",
    "\n",
    "# Quantize the model\n",
    "model_quantized = quantize_model(model, precision=8)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Function to generate text from the model with advanced sampling strategies\n",
    "def generate_text_from_model(model, prompt, max_length=100, temperature=0.7, top_k=50, top_p=0.9):\n",
    "    # Tokenize the input prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to('cpu')\n",
    "\n",
    "    # Move the model to CPU (quantized models are usually on CPU)\n",
    "    model.to('cpu')\n",
    "    \n",
    "    # Generate text using top-k sampling, top-p (nucleus sampling), and adjusted temperature\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs, \n",
    "            max_length=max_length, \n",
    "            temperature=temperature,  # Control the randomness\n",
    "            top_k=top_k,  # Use top-k sampling\n",
    "            top_p=top_p,  # Use nucleus sampling\n",
    "            do_sample=True  # Ensure sampling is used instead of greedy decoding\n",
    "        )\n",
    "    \n",
    "    # Decode the generated tokens back to text\n",
    "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return output_text\n",
    "\n",
    "# Provide a prompt to test the GPT-2 model\n",
    "prompt = \"University of Virginia is a top-ranking public school in the U.S., where  \"\n",
    "\n",
    "# Generate text from the original GPT-2 model with advanced sampling\n",
    "generated_text_fp32 = generate_text_from_model(model, prompt, max_length=100)\n",
    "\n",
    "# Generate text from the quantized GPT-2 model with advanced sampling\n",
    "generated_text_int8 = generate_text_from_model(model_quantized, prompt, max_length=100)\n",
    "\n",
    "# Print the generated text for comparison\n",
    "print(f\"Original GPT-2 Generated text: {generated_text_fp32}\")\n",
    "print(f\"Quantized GPT-2 Generated text: {generated_text_int8}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of Original GPT-2: 25.82721043362427\n",
      "Perplexity of Quantized GPT-2: 3141.5037193211997\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Function to calculate perplexity for a given model and generated text\n",
    "def calculate_perplexity(model, prompt):\n",
    "    # Tokenize the input prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to('cpu')\n",
    "\n",
    "    # Move the model to CPU\n",
    "    model.to('cpu')\n",
    "    \n",
    "    # Get model output (logits)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    \n",
    "    # Compute cross-entropy loss\n",
    "    loss = outputs.loss.item()\n",
    "    \n",
    "    # Compute perplexity from the loss\n",
    "    perplexity = math.exp(loss)\n",
    "    \n",
    "    return perplexity\n",
    "\n",
    "# Calculate perplexity for the original GPT-2 model\n",
    "perplexity_fp32 = calculate_perplexity(model, prompt)\n",
    "\n",
    "# Calculate perplexity for the quantized GPT-2 model\n",
    "perplexity_int8 = calculate_perplexity(model_quantized, prompt)\n",
    "\n",
    "# Print the perplexity scores for comparison\n",
    "print(f\"Perplexity of Original GPT-2: {perplexity_fp32}\")\n",
    "print(f\"Perplexity of Quantized GPT-2: {perplexity_int8}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "posit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
